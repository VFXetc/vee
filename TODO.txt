
- `--link-prefix` so that it doesn't have to install it elsewhere, just link
  it elsewhere.

- use VEE_HOMEBREW=/usr/local so that most bottles just work?
  - this is only a little scary

- it would be wonderful if we could have a git variant which treats it's
  package as its install, or perhaps skips the build, so that we can not
  copy the key_base around every freakin time. OR, --hard-link instead of
  copying. We could do that anyways...
    a) --hard-link uses hard links instead of copies
    b) --one-install 

- PyQt for Maya/Nuke in Linux
  - perhaps we should just do this with envvars?

- `vee package-brew NAME`
    - package up the given homebrew package and all of its dependencies into
      a set (or single, if I feel up to it) tarball.
        - if I made my own, dependencies should be specified as a name/url
          pair, such that the name could be satisfied elsewhere
        - vee-package-meta.yml inside the tarball could describe it
        - vbott extension
    - if there were multiple, it would expand into a series of packages at
      runtime. This could be via Package.provides() ?

- add for loops and string formatting to requirements files
  
  % for x in ...:
  % endfor

  - do not allow git-based packages to be found inside of for loops
  - if statements are ok though

- check config/environ against database when deciding if resolve_existing
  matches. use this for build_subdir and install_prefix too


- it could be nice for packages to register envvars (or scripts to run at source time)
  but it certainly isn't nessesary for us to do up front

- We *could* isolate the various homebrew packages. You must make a
  $PREFIX/bin/brew link to the origin command, and a $PREFIX/Cellar directory.
  Then you can symlink dependencies into $PREFIX/Cellar/$NAME/$VERSIONISH and
  do whatever you want. OR, we can copy things out of the Cellar when they
  are built, and relocate them to their new environment.

- relocate packages

  - `--relocate '@/path/to/map'` gives a specific relocation map, which we need
    for relocating PyQt against Maya's Qt since the names no longer match. 

- very simple automatic persistence model:

    __db_column_map__ = {
        'attrname': 'colname'
    }

    dbobject.rowid or dbobject.persist()
    DBObject.persist_in_db() -> insert or update
    DbObject.delete_from_db() -> delete from database


- Package.dependencies() -> list of Homebrew dependencies, that get installed
  and linked as top-level packages.

- `vee upgrade` after a `vee add` without a push wont find the commit
    - `vee upgrade --dirty` should rewrite GIT urls to pull from dev

- `vee link REQUIREMENT` should use the environment of the default repo

- s/Home/Context

- put `vee.__about__.__revision__` into every database row
- `vee server` and `vee client`
    - make a super simple socket protocol. just send JSON back and forth

- s/Requirement.package/Requirement.new_package()

- linking can link top-level directories. Only when another thing also tries to
  traverse into the same directory, is the original thing broken up into the
  next depth of links, and then recursion continues. This is tricky when you
  need to copy scripts, and link other things.
    - walk the whole tree looking for files that need to be rewritten
    - create a set of all directories which must exist above them
    - do the main walk
    - if a link exists where we need it to be a directory, the break it up by
      turning it into a directory with linked contents
    - OR:
    - just link the directories into place
    - then find everything that needs rewriting
    - break up whatever links you need to
    - rewrite those files

- Homebrew isn't registering installs on first install: "ValueError: cannot record requirement that is not installed"
    - name in the database is "homebrew+x264"; perhaps this is the reason?

- Requirement.package_build_matches() returns if the environment and subdirs
  match between the requirement and package. If it is false, then we can
  force a re-install.

- setting envvars in requirements.txt should allow for @ or $KEY to take
  in previous values that were set within that file. So, we need some syntax for:
    1. Immediately resolution of variables.
    2. Deferred resolution of variables.
    3. Deferred resolution of previous value.


- can vendor macholib and elffile (or patchelf) in order to relocate libs on
  install (via `--relocate`)
    - macholib:
      >>> h = lib.headers[0]
      >>> for c in h.commands: print c[0].get_cmd_name()
    - elffile might implement enough for us to patch it ourselves
    - we can also just depend on chrpath (Linux) and install_name_tool (OSX)




- `vee link [-r REPO] [-e ENV] [-d DIR] [--raw] REQUIREMENT`

- `vee dev checkout --repo REPO` to fastforward everything to match the given repo
- `vee dev ff` to `pull --ff-only` everything to their origin/master

- use difflib to compare the old and new requirements. This will allow for us
  to detect re-orderings. Put dev-only packages at the end. Put dev information
  with the new destination.

- EnvRepo.iter_requirement_versions() -> yields (work, head, dev)

    - Home.get_dev_repos()
    - zip_matching(dev_repos, work_reqs, head_reqs, key=lambda x: x.name)
    - zip_matching(home.get_dev_repos(), env_repo.iter_requirements(), env_repo.iter_requirements(revision='HEAD'), sorted=True, key=lambda x: x.name)

- `vee status` summary at bottom, like `git status`
    Your branch is up-to-date with 'origin/master'.
    nothing to commit, working directory clean

- should dev_packages track remotes (and branches), should that be in the
  git repo itself, or do we do it ad-hoc? We could pick an ideal remote given
  a matching requirement, such that it picks whatever remote we are installing
  from. Everything gets a little easier once we have a DevPackage (which is
  actually a GitRepo).

    - DevPackage.pick_remote_for(Requirement)
        - picks one that matches, and saves it

- `vee push` should also push tools themselves (as long as they match up with
  those in the requirements)

- `vee status` should warn if there aren't remotes that seem related to those
  in the requirements

- server and client: start as a super simple notification server that says when there is
  a new version of any repos


COMMANDS TO WRITE
-----------------

  - vee gc [--installs] [--environs]
      - delete installs (and their DB records) which are not linked to
      - delete installs in DB that don't exist on disk
      - delete anything on disk that isn't referred to by the index
          build a set of relative paths (and all their ancestors), then walk the
          root looking for directories which aren't mentioned, then delete them

  - vee uninstall (NAME|REQUIREMENT [ARGS])
  - vee unlink ENVIRON (NAME|REQUIREMENT [ARGS])

  - vee list
      - list packages, environments, etc..

  - vee freeze [-e ENV] [-R req] [-r repo] [DST_REPO]
      - freeze all requirements into a repository
      - this uses much of the same code from `vee exec` and `vee add`


---

- is it possible for me to bundle libgit2, and build it?

- there are a ton of times we need to guess which dev remote to use; perhaps
  we should have a remote/branch to track in the database, OR figure out how
  to interpret that from the repo itself. Git seems to store a repo per branch,
  so we could always look at the active branch

- RequirementSet elements should be their own class:
    - el.path is path to file that contained it

- merge Requirement into Package

    Package.factory(args)
    make_package(args)

    it is nice having them be separate, and one being user specifications
    perhaps an AbstractPackage would fit the bill

- permissions

    - install_vee.py
        - just warn if they are running as root, but don't stop them
        - default to assuming a single user
        - --multi-user signals that it should try to setup a group setup, OR we
          can just let them figure that out themselves

    - `vee doctor` should do the main permission checks

    - do we need to set our umask, or can permission bits somehow handle that?
        - document the result
    - do we need to chgrp, or can setgid handle that?
        - document the result

- test python packages (for each of source, sdist, bdist, bdist_wheel):
    - they import
    - they import each other
    - console_scripts entrypoints work
    - scripts work
    - their install_requires doesn't matter

- install data that comes with Python wheels
  $NAME-$VERSION.data dir at top-level, beside $NAME-$VERSION.dist-info

- install Python commands listed in egg-info

- linking should check if a previous revision of the same thing was already
  linked into an environment, and stop you

    - Package.resolve_existing(any_revision=True)

- does it make sense to have packages/builds/installs separate in the database,
  instead of having them all together under "packages"

- build-subdir and install-prefix should be used to invalidate matches in the
  resolve_existing

- log everything about the different steps, and stuff it into the database
- CLI IO/API/logging package

    - name:
        x clio (on PyPI)
        - clout

    - styles (copied straight from what we have is cool)
    - io indenting model
        - replaces sys.stdout and sys.stderr, and those are pushed through
        - can spawn reader threads which can (1) buffer the output, (2) echo it, (3) push it to a callback
        - with clout.io.indent(), or clout.io.push_indent() and clout.io.pop_indent()
    - event log
        - format: "$datetime $stream $string-escaped-content"
        - events:
            - exec:$pid -> nth executable
            - arg:$pid:I -> argument I of nth exe
            - out:$pid -> stdout of nth exe
            - err:$pid -> stderr of nth exe

- record stdout/stderr from build process in the database. Use a timestamped
  format: each line starts with "out" or "err" and the timestamp

    out 2015-02-18T15:02:01 sdflkjsdf

- Http/File/Base could do checksums of files to see if they have changed
  - memoize the caches based on inode,size,mtime


LATER
-----

- custom managers
  - e.g. `vee install PyAV.py`, where PyAV.py contains:
        - REQUIREMENT = 'https://pypi.python.org/packages/source/a/av/av-0.2.2.tar.gz#md5=ec0198f28d9294d20b54b0ac3a9ff77d'
        - DEPENDS_ON = ['lib:ffmpeg']
        - MANAGER or PyAVManager or PyavManager, which inherits from BaseManager

- pypi manager
    - PyPI JSON API -> https://pypi.python.org/pypi/%s/json
    - Need to either hit the PyPI on every `.installed` check, or cache versions.

- match how Homebrew links directories, or at least be smarter about it

- homebrew taps: homebrew+mikeboers/testbrew/foo
    - we would need a way to detect which tap it is
    - we can grep `brew info $forumla` for From: https://github.com/#{user}/#{repo}/blob/master/#{path}

- Requirement.dependencies() and Requirement.provisions()

  An AbstractRequirement is one like "lib:ffmpeg", "py:yaml", etc., that just
  know what result they want, but not where it is from. A DependencyInterface
  could be the intersection of AbstractRequirement and Requirement, such that

  Requirement.dependencies() can return real ones (e.g. from `brew deps`)
  and abstract ones. It is permitted to return different dependencies on each
  call (as they are discovered, e.g.)

  DependencyResolver can take a pool of requirements and figure out what order
  they should be installed in (via C3)

        .add(requirement)
        .rescan_dependencies()
        .linearize()


  