

- vee dev_packages should record which user they are for

- how can dev mode work on the farm, if the list of dev_packages is on your
  local home

- bash completions

- vee commit --auto should recate a commit message out of all constituent commit
    messages

- vee-install.sh
    gets VEE_INSTALL_PATH, also $1

- remove write permissions from packages once installed?

- `vee selfupdate` should be as close to just calling the installer as possible
- on OS X we should bootstrap HOMEBREW with python, on linux, we don't need to

- create timestamped commit envionrments too?
    by_hash -> real environments
    by_time -> links timestamped by commit time
    $branch -> links

- install_name for Homebrew is wrong if you have --HEAD (and maybe --devel),
  the version is HEAD

- I really don't like that it is so quick to create a package from a requirement.
  I do like that it is so easy to keep pulling the same package from a given
  requirement, however. So, perhaps another object, e.g. a PackageSet would
  provide this sort of thing.

  PackageSet.resolve(name_or_requirement, reinstall=False) -> Package
  RequirementSet.get(name_or_requirement) -> Requirement
  
- dependency resolution might actually be a thing that we deal with...

    - one is permitted to determine their dependencies at any time; between
      each pipeline step the RequirementSet should inspect all requirements
      for new dependencies, and re-order the set so that they are satisfied.

    - dependencies need to be able to be abstract (such that they may be
      satisfied by the requirement set), or concrete (such that they are
      already a Requirement object). These could either be:

        a) different types, e.g. Requirement and AbstractRequirement
        b) Requirement.type is None signals abstractedness
        c) Package.abstract_requirements is a list of abstract ones

      Ideally, abstract requirements could provide a fallback type that could
      satisfy them. Or, you supply both an abstract or concrete type. OR,
      return (RequirementSet.get_by_name(name) or Requirement('name', type='known_type'))

          check_version_satisfaction(pkg.revision, py_dep_revision)
          or
          Version(py_dep_version).matches(pkg.revision)
          Version('>=2.6').matches('2.7') -> True

          --build-type and --package-type to pick one explicitly, so that
          the dependee can specify them directly

      So, are there any situations in which we cannot either resolve the
      dependency immediately, OR provide a concrete one? Homebrew is immediate
      since it is assumed that its dependencies are Homebrew. Python can
      be immediate since we will make the PyPIPackage (so Requirement('sphinx', type='pypi'))

      Seems like we either need a mechanism to defer resolution of dependencies,
      with a fallback (so provide ('sphinx', 'pypi:sphinx')), or Packages need
      a reference to a RequirementSet (or DependencyPool) so that they can
      resolve them immediately.

      Option 1: Dependency class
          - .name -> for purely abstract stuff
          - .requirement -> for concrete
          - .default_url -> if the abstract can't be resolved
      Option 1b: Use Requirement, and don't access the package attribute.
      Option 2: Packages have a .set
          - self.set.get('name') or Requirement(['url'])

    - Builder.inspect() is run post Package.extract(), and may determine dependencies
        - in python packages, run egg_info and inspect resulting "requires.txt"
        - this could be a part of extract

    - deps would need to be recorded, in order to be linked in a different
      step from installation
        - package_dependencies simply links depender_id to dependee_id
        - this would be loaded by resolve_existing

    - PyPIPackage
        - packages/pypi/NAME/date.json
        - https://pypi.python.org/pypi/NAME/json gives us JSON we can interpret
        - would be create to have version requirements: --revision could be
          an expression: --revision '>=1.0'

    - for Homebrew, we already have homebrew.


- document somewhere that we are treating all names as existing within a global
  namespace. This is particuarly scary given that there are python processes
  that we have no control over, but... meh. Eventually, we could add a namespace
  (e.g. "python_packages", "binaries", etc.)


- `--link-prefix` so that it doesn't have to install it elsewhere, just link
  it elsewhere.

- need to populate environment.repo_id and repo_revision in the database

- PyQt for Maya/Nuke in Linux
  - perhaps we should just do this with envvars?
  - just package up what Mark has preppared previousely in /opt

- `vee bottle -d OUT_DIR PKG_NAME`
    - package up the given homebrew package and all of its dependencies into
      a set (or single, if I feel up to it) tarball.
        - if I made my own, dependencies should be specified as a name/url
          pair, such that the name could be satisfied elsewhere
        - vee-package-meta.yml inside the tarball could describe it
        - vbott extension
    - if there were multiple, it would expand into a series of packages at
      runtime. This could be via Package.provides() ?

- add for loops and string formatting to requirements files
  
  % for x in ...:
  % endfor

  - do not allow git-based packages to be found inside of for loops
  - if statements are ok though

- check config/environ against database when deciding if resolve_existing
  matches. use this for build_subdir and install_prefix too

- it could be nice for packages to register envvars (or scripts to run at source time)
  but it certainly isn't nessesary for us to do up front

- We *could* isolate the various homebrew packages. You must make a
  $PREFIX/bin/brew link to the origin command, and a $PREFIX/Cellar directory.
  Then you can symlink dependencies into $PREFIX/Cellar/$NAME/$VERSIONISH and
  do whatever you want. OR, we can copy things out of the Cellar when they
  are built, and relocate them to their new environment.

- very simple automatic persistence model:

    __db_column_map__ = {
        'attrname': 'colname'
    }

    dbobject.rowid or dbobject.persist()
    DBObject.persist_in_db() -> insert or update
    DbObject.delete_from_db() -> delete from database


- Package.dependencies() -> list of Homebrew dependencies, that get installed
  and linked as top-level packages.

- `vee upgrade` after a `vee add` without a push wont find the commit
    - `vee upgrade --dirty` should rewrite GIT urls to pull from dev

- s/Home/Context

- put `vee.__about__.__revision__` into every database row
- `vee server` and `vee client`
    - make a super simple socket protocol. just send JSON back and forth

- Requirement.package_build_matches() returns if the environment and subdirs
  match between the requirement and package. If it is false, then we can
  force a re-install.

- setting envvars in requirements.txt should allow for @ or $KEY to take
  in previous values that were set within that file. So, we need some syntax for:
    1. Immediately resolution of variables.
    2. Deferred resolution of variables.
    3. Deferred resolution of previous value.

- can vendor macholib and elffile (or patchelf) in order to relocate libs on
  install (via `--relocate`)
    - macholib:
      >>> h = lib.headers[0]
      >>> for c in h.commands: print c[0].get_cmd_name()
    - elffile might implement enough for us to patch it ourselves
    - we can also just depend on chrpath (Linux) and install_name_tool (OSX)

- `vee dev checkout --repo REPO` to fastforward everything to match the given repo
- `vee dev ff` to `pull --ff-only` everything to their origin/master

- use difflib to compare the old and new requirements. This will allow for us
  to detect re-orderings. Put dev-only packages at the end. Put dev information
  with the new destination.

- EnvRepo.iter_requirement_versions() -> yields (work, head, dev)

    - Home.get_dev_repos()
    - zip_matching(dev_repos, work_reqs, head_reqs, key=lambda x: x.name)
    - zip_matching(home.get_dev_repos(), env_repo.iter_requirements(), env_repo.iter_requirements(revision='HEAD'), sorted=True, key=lambda x: x.name)

- `vee status` summary at bottom, like `git status`
    Your branch is up-to-date with 'origin/master'.
    nothing to commit, working directory clean

- should dev_packages track remotes (and branches), should that be in the
  git repo itself, or do we do it ad-hoc? We could pick an ideal remote given
  a matching requirement, such that it picks whatever remote we are installing
  from. Everything gets a little easier once we have a DevPackage (which is
  actually a GitRepo).

    - DevPackage.pick_remote_for(Requirement)
        - picks one that matches, and saves it

- `vee push` should also push tools themselves (as long as they match up with
  those in the requirements)

- `vee status` should warn if there aren't remotes that seem related to those
  in the requirements

- server and client: start as a super simple notification server that says when there is
  a new version of any repos


COMMANDS TO WRITE
-----------------

  - vee gc [--installs] [--environs]
      - delete installs (and their DB records) which are not linked to
      - delete installs in DB that don't exist on disk
      - delete anything on disk that isn't referred to by the index
          build a set of relative paths (and all their ancestors), then walk the
          root looking for directories which aren't mentioned, then delete them

  - vee uninstall (NAME|REQUIREMENT [ARGS])
  - vee unlink ENVIRON (NAME|REQUIREMENT [ARGS])
      - this may not be possible, or desireable

  - vee list
      - list packages, environments, etc..

  - vee freeze [-e ENV] [-R req] [-r repo] [DST_REPO]
      - freeze all requirements into a repository
      - this uses much of the same code from `vee exec` and `vee add`


---

- is it possible for me to bundle libgit2, and build it?

- there are a ton of times we need to guess which dev remote to use; perhaps
  we should have a remote/branch to track in the database, OR figure out how
  to interpret that from the repo itself. Git seems to store a repo per branch,
  so we could always look at the active branch

- RequirementSet elements should be their own class:
    - el.path is path to file that contained it

- merge Requirement into Package

    Package.factory(args)
    make_package(args)

    it is nice having them be separate, and one being user specifications
    perhaps an AbstractPackage would fit the bill

- permissions

    - install_vee.py
        - just warn if they are running as root, but don't stop them
        - default to assuming a single user
        - --multi-user signals that it should try to setup a group setup, OR we
          can just let them figure that out themselves

    - `vee doctor` should do the main permission checks

    - do we need to set our umask, or can permission bits somehow handle that?
        - document the result
    - do we need to chgrp, or can setgid handle that?
        - document the result

- test python packages (for each of source, sdist, bdist, bdist_wheel):
    - they import
    - they import each other
    - console_scripts entrypoints work
    - scripts work
    - their install_requires doesn't matter

- install data that comes with Python wheels
  $NAME-$VERSION.data dir at top-level, beside $NAME-$VERSION.dist-info

- install Python commands listed in egg-info

- does it make sense to have packages/builds/installs separate in the database,
  instead of having them all together under "packages"

- build-subdir and install-prefix should be used to invalidate matches in the
  resolve_existing

- log everything about the different steps, and stuff it into the database
- CLI IO/API/logging package

    - name:
        x clio (on PyPI)
        - clout

    - styles (copied straight from what we have is cool)
    - io indenting model
        - replaces sys.stdout and sys.stderr, and those are pushed through
        - can spawn reader threads which can (1) buffer the output, (2) echo it, (3) push it to a callback
        - with clout.io.indent(), or clout.io.push_indent() and clout.io.pop_indent()
    - event log
        - format: "$datetime $stream $string-escaped-content"
        - events:
            - exec:$pid -> nth executable
            - arg:$pid:I -> argument I of nth exe
            - out:$pid -> stdout of nth exe
            - err:$pid -> stderr of nth exe

- record stdout/stderr from build process in the database. Use a timestamped
  format: each line starts with "out" or "err" and the timestamp

    out 2015-02-18T15:02:01 sdflkjsdf

- Http/File/Base could do checksums of files to see if they have changed
  - memoize the caches based on inode,size,mtime


LATER
-----

- support multiple python versions at the same time, e.g. 2.6, 2.7, 3.4.

- custom managers
  - e.g. `vee install PyAV.py`, where PyAV.py contains:
        - REQUIREMENT = 'https://pypi.python.org/packages/source/a/av/av-0.2.2.tar.gz#md5=ec0198f28d9294d20b54b0ac3a9ff77d'
        - DEPENDS_ON = ['lib:ffmpeg']
        - MANAGER or PyAVManager or PyavManager, which inherits from BaseManager

- pypi manager
    - PyPI JSON API -> https://pypi.python.org/pypi/%s/json
    - Need to either hit the PyPI on every `.installed` check, or cache versions.

- homebrew taps: homebrew+mikeboers/testbrew/foo
    - we would need a way to detect which tap it is
    - we can grep `brew info $forumla` for From: https://github.com/#{user}/#{repo}/blob/master/#{path}

- Requirement.dependencies() and Requirement.provisions()

  An AbstractRequirement is one like "lib:ffmpeg", "py:yaml", etc., that just
  know what result they want, but not where it is from. A DependencyInterface
  could be the intersection of AbstractRequirement and Requirement, such that

  Requirement.dependencies() can return real ones (e.g. from `brew deps`)
  and abstract ones. It is permitted to return different dependencies on each
  call (as they are discovered, e.g.)

  DependencyResolver can take a pool of requirements and figure out what order
  they should be installed in (via C3)

        .add(requirement)
        .rescan_dependencies()
        .linearize()


  