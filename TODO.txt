
- if I can merge Package and Requirememt (by making a BuildPipeline), then
  RequirementSet -> RequirementsFile, and PackageSet can remain.

  Package.pipeline.link(relink=False, reinstall=False)
  Package.pipeline.install(reinstall=False)

  There are two issues:
    1. Attributes on a Requirement are interpreted as requests from the user,
       where the same ones on a Package are interpreted as results.
    2. Pulling from the database.

- development_packages should record which user they are for. Perhaps a
  "dev_set" or "namespace", which defaults to the user, would be better.

- how can dev mode work on the farm, if the list of dev_packages is on your
  local home

- bash completions

- vee commit --auto should recate a commit message out of all constituent commit
    messages

- vee-install.sh
    gets VEE_INSTALL_PATH, also $1

- remove write permissions from packages once installed?

- `vee selfupdate` should be as close to just calling the installer as possible
- on OS X we should bootstrap HOMEBREW with python, on linux, we don't need to

- create timestamped commit envionrments too?
    by_hash -> real environments
    by_time -> links timestamped by commit time
    $branch -> links

- Document Package vs Requirement like PackageSet vs RequirementSet

- PackageSet.auto_install should be named better. Perhaps install and link,
  which are quick wrappers around process(install=True, link=True)


- split up Package into all of the different steps:

  - Pipeline is the machinery
  - PipelineStep is the basic piece which handles the pipeline.

    make_pipeline_step(step, pkg)

    Step.factory(step, pkg)
    step.{fetch,extract,inspect,build,install,develop}()
    step.also_provides('build') -> bool
    step.provide_successor('extract') -> PipelineStep

  
- dependencies

    - instrument revision with VersionExpr to implement Python packages'
      requirements.

    - record these dependencies in the database: package_dependencies
        - this gets loaded by resolve_existing?

    - vee-inspect.sh or vee-requirements.txt to specify requirements, we can
      then immediately create our own "bottles" from Homebrew

        - `vee package-installed name`
            - if there are dependencies, then package those too
            - this requires there to be dependency links for homebrew
              packages


- document somewhere that we are treating all names as existing within a global
  namespace. This is particuarly scary given that there are python processes
  that we have no control over, but... meh. Eventually, we could add a namespace
  (e.g. "python_packages", "binaries", etc.)


- `--link-prefix` so that it doesn't have to install it elsewhere, just link
  it elsewhere.

- need to populate environment.repo_id and repo_revision in the database

- PyQt for Maya/Nuke in Linux
  - perhaps we should just do this with envvars?
  - just package up what Mark has preppared previousely in /opt

- `vee bottle -d OUT_DIR PKG_NAME`
    - package up the given homebrew package and all of its dependencies into
      a set (or single, if I feel up to it) tarball.
        - if I made my own, dependencies should be specified as a name/url
          pair, such that the name could be satisfied elsewhere
        - vee-package-meta.yml inside the tarball could describe it
        - vbott extension
    - if there were multiple, it would expand into a series of packages at
      runtime. This could be via Package.provides() ?

- add for loops and string formatting to requirements files
  
  % for x in ...:
  % endfor

  - do not allow git-based packages to be found inside of for loops
  - if statements are ok though

- check config/environ against database when deciding if resolve_existing
  matches. use this for build_subdir and install_prefix too

- it could be nice for packages to register envvars (or scripts to run at source time)
  but it certainly isn't nessesary for us to do up front

- We *could* isolate the various homebrew packages. You must make a
  $PREFIX/bin/brew link to the origin command, and a $PREFIX/Cellar directory.
  Then you can symlink dependencies into $PREFIX/Cellar/$NAME/$VERSIONISH and
  do whatever you want. OR, we can copy things out of the Cellar when they
  are built, and relocate them to their new environment.

- move save/delete/whatever to the Database or a Session?


- Package.dependencies() -> list of Homebrew dependencies, that get installed
  and linked as top-level packages.

- `vee upgrade` after a `vee add` without a push wont find the commit
    - `vee upgrade --dirty` should rewrite GIT urls to pull from dev

- s/Home/Context

- put `vee.__about__.__revision__` into every database row
- `vee server` and `vee client`
    - make a super simple socket protocol. just send JSON back and forth

- Requirement.package_build_matches() returns if the environment and subdirs
  match between the requirement and package. If it is false, then we can
  force a re-install.

- setting envvars in requirements.txt should allow for @ or $KEY to take
  in previous values that were set within that file. So, we need some syntax for:
    1. Immediately resolution of variables.
    2. Deferred resolution of variables.
    3. Deferred resolution of previous value.

- can vendor macholib and elffile (or patchelf) in order to relocate libs on
  install (via `--relocate`)
    - macholib:
      >>> h = lib.headers[0]
      >>> for c in h.commands: print c[0].get_cmd_name()
    - elffile might implement enough for us to patch it ourselves
    - we can also just depend on chrpath (Linux) and install_name_tool (OSX)

- `vee dev checkout --repo REPO` to fastforward everything to match the given repo
- `vee dev ff` to `pull --ff-only` everything to their origin/master

- use difflib to compare the old and new requirements. This will allow for us
  to detect re-orderings. Put dev-only packages at the end. Put dev information
  with the new destination.

- EnvRepo.iter_requirement_versions() -> yields (work, head, dev)

    - Home.get_dev_repos()
    - zip_matching(dev_repos, work_reqs, head_reqs, key=lambda x: x.name)
    - zip_matching(home.get_dev_repos(), env_repo.iter_requirements(), env_repo.iter_requirements(revision='HEAD'), sorted=True, key=lambda x: x.name)

- `vee status` summary at bottom, like `git status`
    Your branch is up-to-date with 'origin/master'.
    nothing to commit, working directory clean

- should dev_packages track remotes (and branches), should that be in the
  git repo itself, or do we do it ad-hoc? We could pick an ideal remote given
  a matching requirement, such that it picks whatever remote we are installing
  from. Everything gets a little easier once we have a DevPackage (which is
  actually a GitRepo).

    - DevPackage.pick_remote_for(Requirement)
        - picks one that matches, and saves it

- `vee push` should also push tools themselves (as long as they match up with
  those in the requirements)

- `vee status` should warn if there aren't remotes that seem related to those
  in the requirements

- server and client: start as a super simple notification server that says when there is
  a new version of any repos


COMMANDS TO WRITE
-----------------

  - vee gc [--installs] [--environs]
      - delete installs (and their DB records) which are not linked to
      - delete installs in DB that don't exist on disk
      - delete anything on disk that isn't referred to by the index
          build a set of relative paths (and all their ancestors), then walk the
          root looking for directories which aren't mentioned, then delete them

  - vee uninstall (NAME|REQUIREMENT [ARGS])
  - vee unlink ENVIRON (NAME|REQUIREMENT [ARGS])
      - this may not be possible, or desireable

  - vee list
      - list packages, environments, etc..

  - vee freeze [-e ENV] [-R req] [-r repo] [DST_REPO]
      - freeze all requirements into a repository
      - this uses much of the same code from `vee exec` and `vee add`


---

- is it possible for me to bundle libgit2, and build it?

- there are a ton of times we need to guess which dev remote to use; perhaps
  we should have a remote/branch to track in the database, OR figure out how
  to interpret that from the repo itself. Git seems to store a repo per branch,
  so we could always look at the active branch

- RequirementSet elements should be their own class:
    - el.path is path to file that contained it

- merge Requirement into Package

    Package.factory(args)
    make_package(args)

    it is nice having them be separate, and one being user specifications
    perhaps an AbstractPackage would fit the bill

- permissions

    - install_vee.py
        - just warn if they are running as root, but don't stop them
        - default to assuming a single user
        - --multi-user signals that it should try to setup a group setup, OR we
          can just let them figure that out themselves

    - `vee doctor` should do the main permission checks

    - do we need to set our umask, or can permission bits somehow handle that?
        - document the result
    - do we need to chgrp, or can setgid handle that?
        - document the result

- test python packages (for each of source, sdist, bdist, bdist_wheel):
    - they import
    - they import each other
    - console_scripts entrypoints work
    - scripts work
    - their install_requires doesn't matter

- install data that comes with Python wheels
  $NAME-$VERSION.data dir at top-level, beside $NAME-$VERSION.dist-info

- install Python commands listed in egg-info

- does it make sense to have packages/builds/installs separate in the database,
  instead of having them all together under "packages"

- build-subdir and install-prefix should be used to invalidate matches in the
  resolve_existing

- log everything about the different steps, and stuff it into the database
- CLI IO/API/logging package

    - name:
        x clio (on PyPI)
        - clout

    - styles (copied straight from what we have is cool)
    - io indenting model
        - replaces sys.stdout and sys.stderr, and those are pushed through
        - can spawn reader threads which can (1) buffer the output, (2) echo it, (3) push it to a callback
        - with clout.io.indent(), or clout.io.push_indent() and clout.io.pop_indent()
    - event log
        - format: "$datetime $stream $string-escaped-content"
        - events:
            - exec:$pid -> nth executable
            - arg:$pid:I -> argument I of nth exe
            - out:$pid -> stdout of nth exe
            - err:$pid -> stderr of nth exe

- record stdout/stderr from build process in the database. Use a timestamped
  format: each line starts with "out" or "err" and the timestamp

    out 2015-02-18T15:02:01 sdflkjsdf

- Http/File/Base could do checksums of files to see if they have changed
  - memoize the caches based on inode,size,mtime


LATER
-----

- support multiple python versions at the same time, e.g. 2.6, 2.7, 3.4.

- custom managers
  - e.g. `vee install PyAV.py`, where PyAV.py contains:
        - REQUIREMENT = 'https://pypi.python.org/packages/source/a/av/av-0.2.2.tar.gz#md5=ec0198f28d9294d20b54b0ac3a9ff77d'
        - DEPENDS_ON = ['lib:ffmpeg']
        - MANAGER or PyAVManager or PyavManager, which inherits from BaseManager

- pypi manager
    - PyPI JSON API -> https://pypi.python.org/pypi/%s/json
    - Need to either hit the PyPI on every `.installed` check, or cache versions.

- homebrew taps: homebrew+mikeboers/testbrew/foo
    - we would need a way to detect which tap it is
    - we can grep `brew info $forumla` for From: https://github.com/#{user}/#{repo}/blob/master/#{path}

- Requirement.dependencies() and Requirement.provisions()

  An AbstractRequirement is one like "lib:ffmpeg", "py:yaml", etc., that just
  know what result they want, but not where it is from. A DependencyInterface
  could be the intersection of AbstractRequirement and Requirement, such that

  Requirement.dependencies() can return real ones (e.g. from `brew deps`)
  and abstract ones. It is permitted to return different dependencies on each
  call (as they are discovered, e.g.)

  DependencyResolver can take a pool of requirements and figure out what order
  they should be installed in (via C3)

        .add(requirement)
        .rescan_dependencies()
        .linearize()


  